---
title: "Introduction to sframe"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to sframe}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

<center> <h1>Introduction to sframe</h1> </center>

R's `data.frames` are stored entirely in memory, and operations typically make several copies, which means the analysis of larger data sets requires machines with a ton of RAM or very careful attention to the objects you have in your environment and the operations you make. This brings very difficult scalability problems

Using Dato's scalable data frame, the `sframe`, your data analysis tasks can scale into the terabyte range, even on your laptop. We try our best to optimize the `sframe`:

* **Memory**: A `sframe` is effectively a data frame that sits on your hard disk rather than in memory.

* **Parallelization**: Custom column transformations are performed using as many CPUs as possible. This lets you immediately take full advantage of the hardware you have.

* **Group-by, sort, and join**: These are done in a way that make intelligent use of memory -- so that you don’t have to.

* **Sparse data**: Check out our stack/unstack feature that lets you take row-based lists and concatenate them together into a “long”-format. We've found these extremely useful and flexible for feature engineering.

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
```
# Data Engineering

A very small subset of airline data borrowed from [Introduction to dplyr](https://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html) has been included in the package. It is used to show quick examples. We highly encourage users to play our tools with really large data set to see its power.

```{r}
library(sframe)
flight_csv <- system.file("extdata", "flights.csv", package = "sframe")
flights <- load.csv(flight_csv, header = TRUE, sep = ",")
```

When applying a filter on the `sframe`, you may notice `Rows: ?`.
This is due to our lazy evaluation strategey to save resource.

```{r}
# dplyr:  flights[flights$month == 1 & flights$day == 1, ]
# pandas: flights[(flights.month == 1) & (flights.day == 1)]
flights[flights['month'] == 1 & flights['day'] == 1]
```

You can save the sframe in csv or binary, the default and recommended format is binary.

```{r}
save.sframe(flights, filename = "flights_binary")
```

We support very user-friendly operations on sframe, like `group_by` below:

```{r}
# dplyr: 
# daily <- group_by(flights, year, month, day)
# (per_day   <- summarise(daily, flights = n()))
# pandas: 
# daily = flights.groupby(['year', 'month', 'day'])
# per_day = daily['distance'].count()
per_day <- group_by(flights, group_keys = c('year', 'month', 'day'), per_day = count(distance))
per_day
```

`stack_columns`/`unstack_columns` are extremely useful and flexible for feature engineering.

```{r}
flights_day <- flights[c("day", "flight")]
flights_day_unstack <- unstack_columns(flights_day, "flight", "flights")
flights_day_unstack
```

Due to parallelization backend, you may need to resort by one column. `magrittr` will be your friend.

```{r}
library(magrittr)
flights_day_unstack <- unstack_columns(flights_day, "flight", "flights") %>% sort_by("day")
flights_day_unstack
```

You can write your own lambda function and apply it on each row in a sframe.
The lambda function takes a list as input and you can add the result back to the sframe on the fly.

```{r}
avg_speed <- function(x) {
  return(x[['distance']]/(x[['arr_time']] - x[['dep_time']]))
}
speed <- sfapply(flights, avg_speed)
flights['speed'] <- speed
flights
```

# Visualization

We think it is quite hard to develop a visualization tool as good as `ggplot2`.
You can use the same syntax on sframe.
The diamonds data set is used here to make you feel more familiar.
To save resource, no more than 5000 data points will plot by default.

```{r}
diamonds_csv <- system.file("extdata", "diamonds.csv", package = "sframe")
diamonds <- load.csv(diamonds_csv, header = TRUE, sep = ",")
qplot(log(carat), log(price), data = diamonds)
qplot(carat, price, data = diamonds, colour = color)
```

# SDK for extension

The higgs-twitter graph data is used here and we develop a weighted pagerank on it.

```{r}
higgs_edge_csv <- system.file("extdata", "higgs_edge.csv", package = "sframe")
higgs_vertex_csv <- system.file("extdata", "higgs_vertex.csv", package = "sframe")
higgs_edge <- load.csv(higgs_edge_csv, header = TRUE, sep = ",")
higgs_vertex <- load.csv(higgs_vertex_csv, header = TRUE, sep = ",")
higgs_sgraph <- sgraph(higgs_vertex, higgs_edge)
higgs_sgraph
```

The key part of the SDK is the `Rcpp::as` and `Rcpp::wrap`,
which provide the type conversion between R types and C++ types.
In the example below, `Rcpp::as<graphlab::gl_sgraph>` takes a `sgraph` from R 
and converts it into `gl_sgraph` in C++; `Rcpp::wrap` takes a `gl_sgraph` from C++
and converts it back to a `sgraph` in R.
This also provides the possibility to develop new packages based on `sframe`,
see `sframe.package.skeleton` for details.

```{r}
library(inline)
code <- '
  graphlab::gl_sgraph g = Rcpp::as<graphlab::gl_sgraph>(sg);
  int iters = Rcpp::as<int>(num_iters);
  std::string weight_field = Rcpp::as<std::string>(field);
  const double RESET_PROB=0.15;

  // Get rid of unused fields.
  graphlab::gl_sgraph g_min = g.select_fields({weight_field});

  // Count the total out going weight of each vertex into an SFrame.
  graphlab::gl_sframe outgoing_weight = g_min.get_edges().groupby({"__src_id"}, 
    {{"total_weight", graphlab::aggregate::SUM(weight_field)}});

  // Add the total_weight to the graph as vertex atrribute.
  // We can update the vertex data by adding the same vertex.
  graphlab::gl_sgraph g2 = g_min.add_vertices(outgoing_weight, "__src_id");

  // Lambda function for triple_apply
  auto pr_update = [&weight_field](graphlab::edge_triple& triple)->void {
    double weight = triple.edge[weight_field];
    triple.target["pagerank"] += triple.source["pagerank_prev"] * weight / triple.source["total_weight"];
  };

  // Initialize pagerank value
  g2.vertices()["pagerank_prev"] = 1.0;

  // Iteratively run triple_apply with the update function
  for (size_t i = 0; i < iters; ++i) {

    g2.vertices()["pagerank"] = 0.0;

    Rcpp::Rcout << "Iteration " << (i+1) << std::endl;
    g2 = g2.triple_apply(pr_update, {"pagerank"});

    g2.vertices()["pagerank"] = RESET_PROB + (1-RESET_PROB) * g2.vertices()["pagerank"];
    g2.vertices()["pagerank_prev"] = g2.vertices()["pagerank"];
  }

  g2.vertices().remove_column("pagerank_prev");
  g2.vertices().remove_column("total_weight");
  return Rcpp::wrap(g2);
'
weighted_pagerank <- cxxfunction(signature(sg = "SEXP", num_iters = "SEXP", 
                                   field = "character"), code, plugin = "sframe")
```

Then we have a function `weighted_pagerank`.

```{r}
g2 <- weighted_pagerank(higgs_sgraph, num_iters = 10, field = "X3")
g2
edges(g2)
vertices(g2)
```

